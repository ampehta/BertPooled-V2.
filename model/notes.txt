1. MultiHeadAttention 매번 d_model 조정해주기 ( pool 된 hidden_size에 맞춰서)
2. d_model%head_num 조심 -> d_model ==768 pool_size--[1,2,4,8,16]이면 문제 없음
3. 패딩 후 attention_mask -> 마스크도 풀링을 해버릴까
4. 나중에 head_num 뒤로 갈수록 늘어나도록 추가
